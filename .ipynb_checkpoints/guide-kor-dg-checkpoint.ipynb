{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "171cf56936f07fe2054ed5aff12757275129f19b"
   },
   "source": [
    "# 데이터 분석 어떻게 시작해야 하나요?\n",
    "[GUIDE, KOR, DG] How do I begin this analysis....\n",
    "\n",
    "---\n",
    "\n",
    "본 커널은, 2019 ML month 대구 튜토리얼을 위한 커널입니다.\n",
    "\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "0. 데이터 분석 시작하기 앞서서\n",
    "1. 데이터 셋 확인\n",
    "2. 탐색적 데이터 분석 (EDA, Exploratory Data Analysis)\n",
    "3. 특성 공학 (Feature Engineering)\n",
    "4. 모델 개발 및 학습\n",
    "    1. scikit-learn을 사용한 RandomForest모델 개발\n",
    "    1. keras를 사용한 NN 모델 개발 (미완)\n",
    "5. 모델 예측 및 평가\n",
    "    1. RandomForest 예측 및 평가\n",
    "    1. NN 예측 및 평가 (미완)\n",
    "6. 결론\n",
    "7. 어떻게 공부하는게 좋을까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d95efc8eb5f14cb276764c16b96a60ebb5ddb7de"
   },
   "source": [
    "## References\n",
    "\n",
    "해당 커널은 아래의 멋진 분석 커널을 참고하였습니다.  \n",
    "멋진 분석을 제공해준 모든 분들께 감사드립니다. 항상 많은 도움이 되고 있습니다.\n",
    "\n",
    "* [colab, 전태균님의 타이타닉 분석](https://colab.research.google.com/drive/1cqv5yD9uLHHrVFL-TGM9NPSD1ZyF4AC1#scrollTo=jwBNmHcw3w91)\n",
    "* [블로그, 이유한님의 타이타닉 분석 튜토리얼](http://kaggle-kr.tistory.com/17?category=821486)\n",
    "* [공개 커널, Keras를 이용한 간단한 모델 구현!](https://www.kaggle.com/everystep/keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c7631ad0ddedb6c6c4d76776c11f8c1aa1a4d52a"
   },
   "source": [
    "## 0. 데이터 분석하기 앞서서\n",
    "---\n",
    "### 이 튜토리얼은 캐글을 시작하기 위한 가이드 입니다.  \n",
    "\n",
    "\"데이터를 이런식으로 가지고 놀 수 있구나~\" 정도의 수준입니다.  \n",
    "이 튜토리얼로 데이터 분석 전반적인 흐름을 잡고 어떤 식으로  \n",
    "공부해야 할지 가이드를 잡아 드리는 것이 목표입니다.  \n",
    "부족한 부분이 있으면 언제든 피드백 부탁드립니다! 항상 배우고 싶습니다..\n",
    "\n",
    "꼭 튜토리얼이 끝나시면, 개인적으로 다른 분들의 커널을 보시고  \n",
    "따라 해보시면서 다양하고 재밌는 인사이트를 얻으시길 기원합니다!  \n",
    "즐거운 캐글 되세요!\n",
    "\n",
    "### 일단 따라서 작성하고 제출해보세요!\n",
    "*\"Done is better than perfect!\"*  \n",
    "\n",
    "사람마다 학습하고 적용하는 방법은 다르겠지만, 저는 먼저  \n",
    "해보고 재미를 느끼는 부류(?)입니다. 안에 있는 코드를 모두 이해하고  \n",
    "개발하기엔 너무 공부해야 할 양이 많으며 시작이 느려지고   \n",
    "동기가 약해지는 경험을 했습니다.\n",
    "\n",
    "그래서 추천하자면, 기본과 실전을 동시에 하라고 추천드리고 싶습니다.  \n",
    "캐글은 개인이 분석한 커널을 공개하는 문화가 잘 형성되어 있습니다.  \n",
    "해당 커널을 따라해보고 제출을 해보고 모델을 튜닝하고 다시 제출하고...  \n",
    "이렇게 각종 점수를 눈에서 보면서 개발하니 재미를 느낄 수 있었습니다.  \n",
    "\n",
    "일단 따라서 작성하고 제출하고, 점수를 눈으로 보시면 재미를 느낄 수 있으실거라 생각합니다!!\n",
    "\n",
    "### 왜 처음 시작이 타이타닉 이죠?\n",
    "\n",
    "만약 데이터 사이언스, 머신러닝 또는 캐글에서 어떤 것을 해야하는 지  \n",
    "잘 모르는 newbie 라면, 타이타닉을 하시는 게 가장 좋은 선택입니다.  \n",
    "데이터 분석 전체 프로세스를 빠르게 경험하실 수 있으며,  \n",
    "접근하기 좋은 Binary Classification (0 혹은 1을 예측하는) 문제이기 때문입니다.\n",
    "\n",
    "### 본 튜토리얼에선 파이썬의 다음과 같은 도구를 사용할 것입니다.\n",
    "\n",
    "* 여러 시각화 도구 : matplotlib, seaborn, plotly\n",
    "* 데이터 분석 도구 : pandas, numpy\n",
    "* 모델 개발 도구 : sklearn, keras\n",
    "\n",
    "###  해당 커널은 다음과 같은 프로세스로 진행됩니다.\n",
    "\n",
    "1. 데이터 셋 확인\n",
    "    * 데이터가 어떻게 구성되어 있는지 확인합니다.\n",
    "    * 대부분의 캐글 데이터들은 잘 정제되어 있습니다.   \n",
    "    하지만 가끔 null data가 존재합니다. 이를 확인하고, 향후 수정합니다.\n",
    "2. 탐색적 데이터 분석 (EDA, Exploratory Data Analysis)\n",
    "    * 여러 feature 들을 개별적으로 분석하고, feature 들 간의 상관관계를 확인합니다.  \n",
    "    여러 시각화 툴을 사용하여 insight를 얻습니다.\n",
    "3. 특성 공학 (Feature Engineering)\n",
    "    * 모델을 세우기에 앞서, 모델의 성능을 높일 수 있도록 feature 들을 engineering 합니다.  \n",
    "    one-hot encoding, class로 나누기, 구간으로 나누기, 텍스트 데이터 처리 등을 합니다.\n",
    "4. 모델 개발 및 학습\n",
    "    * sklearn, keras 을 사용해 모델을 만듭니다. 파이썬에서 머신러닝을 할 때는 sklearn 을  \n",
    "    사용하면 수많은 알고리즘을 일관된 문법으로 사용할 수 있습니다. 또 keras는 딥러닝 개발  \n",
    "    할 때 모델에 집중할 수 있도록 해줍니다. 물론 딥러닝을 위해 tensorflow, pytorch 등을   \n",
    "    사용할 수 도 있습니다. 그리고 학습된 모델이 어떤 것을 학습하였는 지 확인해봅니다\n",
    "5. 모델 예측 및 평가\n",
    "    * Train set 을 가지고 모델을 학습시킨 후, Test set 을 가지고 prediction 합니다.  \n",
    "    그리고 예측 성능이 원하는 수준인지 판단합니다.  \n",
    "    풀려는 문제에 따라 모델을 평가하는 방식도 달라집니다.\n",
    "    \n",
    "### 그럼 시작해 보도록 하겠습니다!\n",
    "\n",
    "> **Tip**:\n",
    "> 그대로 따라 써보는 것도 좋지만, 궁금하면 `print()`로 출력해보고 확인 해보시는 습관을 가지는게 좋은 것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b0ae14b6c967f8cd832588fb9ef2813f9f4dc70"
   },
   "source": [
    "## 1. 데이터 셋 확인\n",
    "---\n",
    "일단 필요한 패키지(라이브러리)를 import 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c0a4f688375dd2a7c512c9247475ee421b88b49"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import sklearn\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set(font_scale=2.5) # 이 두줄은 본 필자가 항상 쓰는 방법입니다. matplotlib 의 기본 scheme 말고 seaborn scheme 을 세팅하고, 일일이 graph 의 font size 를 지정할 필요 없이 seaborn 의 font_scale 을 사용하면 편합니다.\n",
    "import missingno as msno\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # 워닝 메세지를 생략해 줍니다. 차후 버전관리를 위해 필요한 정보라고 생각하시면 주석처리 하시면 됩니다.\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "os.listdir(\"../input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e49335558ea8fb0308137947a878951fb200175"
   },
   "source": [
    "input 경로에 있는 파일 리스트를 확인해 보니 위와 같이 3개가 나옴을 볼 수 있습니다.  \n",
    "각 파일의 역할은 다음과 같습니다.\n",
    "\n",
    "* `train.csv` : 학습에 이용하는 파일 입니다.\n",
    "* `test.csv` : 우리가 학습한 모델을 가지고 예측해야하는 파일입니다.\n",
    "* `sample_submission.csv` : 예측하고 해당 파일 폼에 맞추어 캐글에 제출해야 합니다.\n",
    "\n",
    "`pandas`를 사용해 각 파일을 로드해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../titanic/train.csv')\n",
    "df_test = pd.read_csv('../titanic/test.csv')\n",
    "df_submit = pd.read_csv('../titanic/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "374c4b109aaebf2461ff342d4ca2392c4bcc10be"
   },
   "outputs": [],
   "source": [
    "df_train.shape, df_test.shape, df_submit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f4a8f8d1366025b2e1b2d6f5128d5762e7cb36ca"
   },
   "source": [
    "각 파일을 로드하고 각 파일의 크기를 확인해 보니,  \n",
    "train할 데이터 수는 891개,  \n",
    "test(제출해야)할 데이터 수는 418개를 확인해 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "68a51233108831a95c5b90cba0d3801558fbf050"
   },
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd7eb289073f3813e26ed2400b6254d33af73670"
   },
   "source": [
    "column(feature)는 총 12개로 이루어져 있으며,  \n",
    "학습에 사용해야 할 feature는 11개,  \n",
    "예측해야 할 feature는 `Survived`입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "351225ee9f655f854ef0ecc05117666a5f4bf284"
   },
   "outputs": [],
   "source": [
    "df_submit.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ac845a22f0e8625f7eb96c1fe053dd5a4459d5d"
   },
   "source": [
    "submit 파일은 2개의 칼럼만 있으면 됩니다.  \n",
    "어떤 사람인지 구분하는 `PassengerID`와,  \n",
    "해당 사람이 살았는지 죽었는지 구분하는 `Survived`만 제출하면 됩니다.\n",
    "\n",
    "각 파일을 한번 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ecbe4fa5e9565cc1d61d327642b747c4141a0237"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f33558c9139ddc6d40c6fefbd70f94357e41c6b"
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "79d55476f3fa3b48582ccae4ef9c65e200c27c84"
   },
   "outputs": [],
   "source": [
    "df_submit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d668408a426302ab94a00669c7e8f1b7771e08cb"
   },
   "source": [
    "우리가 다루는 문제에서 feature는 Pclass, Age, SibSp, Parch, Fare 이며, 예측하려는 target label 은 Survived 입니다.\n",
    "\n",
    "### 타이타닉 데이터 Feature 설명 \n",
    "\n",
    "* survival - 생존유무, target 값. (0 = 사망, 1 = 생존)\n",
    "* pclass - 티켓 클래스. (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "* sex - 성별\n",
    "* Age - 나이(세)\n",
    "* sibsp - 함께 탑승한 형제자매, 배우자 수 총합\n",
    "* parch - 함께 탑승한 부모, 자녀 수 총합\n",
    "* ticket - 티켓 넘버\n",
    "* fare - 탑승 요금\n",
    "* cabin - 객실 넘버\n",
    "* bembarked - 탑승 항구\n",
    "\n",
    "### 각 Feature types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9d34ed7b95814b2aa3eb82db0feba61396925ab0"
   },
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b571fdebbe0be5cec523506bc651b9ab27ab2628"
   },
   "source": [
    "pandas dataframe 에는 describe() 메소드가 있는 데, 이를 쓰면 각 feature 가 가진 통계치들을 반환해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e53215d81c10f08211691aaa63d01303de1a7a43"
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d3c698bc46c1b5cfbf0ece26e579f48b496dc91c"
   },
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e5babea1bb65716011c3b610bbca5df96f5d4284"
   },
   "source": [
    "흠.. 데이터를 보니 칼럼 count값이 다른 feature가 보입니다.  \n",
    "결측치(Null)가 있는 것으로 보이는데 한번 알아보도록 하겠습니다.\n",
    "\n",
    "### **1.1 결측치(Null Data) 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "03ba19b56b2f590cdae93cc1f9649642d0f42346"
   },
   "outputs": [],
   "source": [
    "df_train.isnull().sum() / df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "97a565bfab1d6016a8d9fa159a2db92d9807df92"
   },
   "outputs": [],
   "source": [
    "df_test.isnull().sum() / df_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5e813b5ecbb7a23c201772b5f5187c8ea79b2aa"
   },
   "source": [
    "train set과 test set에 비슷하게 `Age` 피쳐에 약 20%, `Cabin` 피쳐에 약 80%의  \n",
    "결측치가 있음을 확인할 수 있습니다.\n",
    "\n",
    "### **1.2 Target Label, `Survived` 확인**\n",
    "\n",
    "target label 이 어떤 distribution 을 가지고 있는 지 확인해봐야 합니다.  \n",
    "지금 같은 binary classification 문제의 경우에서,  \n",
    "1과 0의 분포가 어떠냐에 따라 모델의 평가 방법이 달라 질 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0894c0f7774983506ee3dd0db63252ab09940c7a"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "df_train['Survived'].value_counts().plot.pie(explode=[0, 0.1], autopct='%1.1f%%', ax=ax[0], shadow=True)\n",
    "ax[0].set_title('Pie plot - Survived')\n",
    "ax[0].set_ylabel('')\n",
    "sns.countplot('Survived', data=df_train, ax=ax[1])\n",
    "ax[1].set_title('Count plot - Survived')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "84d9d6d648c381c5a48e37982a64c8cff2f677c5"
   },
   "source": [
    "target label 의 분포가 제법 균일(balanced)합니다.  \n",
    "불균일한 경우, 예를 들어서 100중 1이 99, 0이 1개인 경우에는  \n",
    "만약 모델이 모든것을 1이라 해도 정확도가 99%가 나오게 됩니다.  \n",
    "0을 찾는 문제라면 이 모델은 원하는 결과를 줄 수 없게 됩니다.  \n",
    "지금 문제에서는 그렇지 않으니 계속 진행하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be401bfa9ef54ef3edd1f0ab5dfb688db6fd4303"
   },
   "source": [
    "## 2. 탐색적 데이터 분석 (EDA, Exploratory Data Analysis)\n",
    "---\n",
    "이제 본격적으로 데이터 분석을 해보겠습니다.  \n",
    "데이터는 매우 많습니다. 이 많은 데이터 안에 숨겨진  \n",
    "사실을 찾기 위해선 적절한 시각화가 필요합니다.\n",
    "\n",
    "시각화 라이브러리는 matplotlib, seaborn, plotly 등이 b있습니다.  \n",
    "특정 목적에 맞는 소스코드를 정리해두어 필요할 때마다 참고하면 편합니다\n",
    "\n",
    "### **2.1 Pcalss**\n",
    "먼저 Pclass 에 대해서 살펴보겠습니다.\n",
    "\n",
    "* Pclass는 서수형 데이터입니다.  \n",
    "즉, 카테고리이면서 순서가 있는 데이터 타입입니다.  \n",
    "먼저 Pclass 에 따른 생존률의 차이를 살펴보겠습니다.  \n",
    "* 엑셀의 피벗 차트와 유사한 작업을 하게 되는데,  \n",
    "pandas dataframe 에서는 `groupby` 를 사용하면 쉽게 할 수 있습니다.  \n",
    "또한 `pivot` 이라는 메소드도 있습니다.  \n",
    "* `Pclass`,`Survived` 를 가져온 후, pclass 로 묶습니다.  \n",
    "그러고 나면 각 pclass 마다 0, 1 이 count가 되는데,  \n",
    "이를 평균내면 각 pclass 별 생존률이 나옵니다\n",
    "* 아래와 같이 count() 를 하면, 각 class 에 몇명이 있는 지  \n",
    "확인할 수 있으며, sum() 을 하면, 216 명중 생존한(survived=1)  \n",
    "사람의 총합을 주게 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "814bdf165f5ee12609727aada2b6673c7cf60a03"
   },
   "outputs": [],
   "source": [
    "# pclass 그룹 별 데이터 카운트\n",
    "df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=True).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "501ba7b503f28c3b499721c9e1a01be24dc87ac3"
   },
   "outputs": [],
   "source": [
    "# pclass 그룹 별 생존자 수 합\n",
    "df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bab1ad4d511ab5fccbe22ec004b9761688a1da67"
   },
   "outputs": [],
   "source": [
    "# 위와 같은 작업을 crosstab으로 편하게 할 수 있습니다.\n",
    "pd.crosstab(df_train['Pclass'], df_train['Survived'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f0f3ce424484030a4ce0cd0aae162ba1969ef0b"
   },
   "outputs": [],
   "source": [
    "# mean은 생존률을 구하게 할 수 있습니다.\n",
    "df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b0177ab7bdfddaa4bfcbcd4a6fc441ae46680af"
   },
   "outputs": [],
   "source": [
    "# 이를 시각화 해보았습니다.\n",
    "df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=True).mean().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de99d2520ea37ec81a97692c1cc7c2912dda5f57"
   },
   "source": [
    "우리는 생존에 Pclass 가 큰 영향을 미친다고 생각해볼 수 있으며,  \n",
    "나중에 모델을 세울 때 이 feature 를 사용하는 것이 좋을 것이라 판단할 수 있습니다.\n",
    "\n",
    "### **2.2 Sex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c59afe24af7ab9203059405e11d92086386c0964"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(18, 8))\n",
    "df_train[['Sex', 'Survived']].groupby(['Sex'], as_index=True).mean().plot.bar(ax=ax[0])\n",
    "ax[0].set_title('Survived vs Sex')\n",
    "sns.countplot('Sex', hue='Survived', data=df_train, ax=ax[1])\n",
    "ax[1].set_title('Sex: Survived vs Dead')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "615b2cf900ebefd3f6eb1385e5af780db5b06090"
   },
   "source": [
    "보시다시피, 여자가 생존할 확률이 높습니다.  \n",
    "Pclass 와 마찬가지로, Sex 도 예측 모델에 쓰일 중요한 feature 임을 알 수 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "98a5fe8047adfe788858d0cc6f21613a5aa20305"
   },
   "source": [
    "### **2.3 Both Sex and Pclass**\n",
    "\n",
    "이번에는 Sex, Pclass 두가지에 관하여 생존이 어떻게 달라지는 지 확인해 봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27880053a936a829d44ab87903768017a13076d1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.factorplot('Pclass', 'Survived', hue='Sex', data=df_train, \n",
    "               size=6, aspect=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c726273ab67efc9e5d0afae384996ffc84cf5595"
   },
   "source": [
    "모든 클래스에서 female 이 살 확률이 male 보다 높은 걸 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90a0064fd67a66f9ff0897058e7e4ec565b9dd05"
   },
   "source": [
    "### **2.4 Age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c27d887233b0f2a95993929e56d99f6c4e876ec4"
   },
   "outputs": [],
   "source": [
    "print('제일 나이 많은 탑승객 : {:.1f} Years'.format(df_train['Age'].max()))\n",
    "print('제일 어린 탑승객 : {:.1f} Years'.format(df_train['Age'].min()))\n",
    "print('탑승객 평균 나이 : {:.1f} Years'.format(df_train['Age'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f07b1dd3032dfcefe690f2e0d8eaab6c03ee4c6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(9, 5))\n",
    "sns.kdeplot(df_train[df_train['Survived'] == 1]['Age'], ax=ax)\n",
    "sns.kdeplot(df_train[df_train['Survived'] == 0]['Age'], ax=ax)\n",
    "plt.legend(['Survived == 1', 'Survived == 0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b7f001a1987efd39e8404dc93f60c859be320329"
   },
   "source": [
    "해당 히스토그램을 보시면 어린 나이(약 15세 미만) 구간에서  \n",
    "생존률이 상대적으로 높음을 확인할 수 있습니다.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ac81d45029ba3cb126b7531b7c4882d1d4ba3758"
   },
   "outputs": [],
   "source": [
    "# Age distribution withing classes\n",
    "plt.figure(figsize=(8, 6))\n",
    "df_train['Age'][df_train['Pclass'] == 1].plot(kind='kde')\n",
    "df_train['Age'][df_train['Pclass'] == 2].plot(kind='kde')\n",
    "df_train['Age'][df_train['Pclass'] == 3].plot(kind='kde')\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.title('Age Distribution within classes')\n",
    "plt.legend(['1st Class', '2nd Class', '3rd Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b2f1b10213469794a4d039fd5c9ed8139acd40e"
   },
   "source": [
    "위의 그래프 클래스가 높아질 수록, 나이 연령대가 높아짐을 확인할 수 있습니다.  \n",
    "\n",
    "나이대가 높아질 수록 생존확률이 어떻게 달라지는지 알고 싶어져,  \n",
    "누적 확률을 활용한 시각화를 해보았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f885359c808c23e68ca15748b64aee4dfe66cbad"
   },
   "outputs": [],
   "source": [
    "cummulate_survival_ratio = []\n",
    "for i in range(1, 80):\n",
    "    cummulate_survival_ratio.append(df_train[df_train['Age'] < i]['Survived'].sum() / len(df_train[df_train['Age'] < i]['Survived']))\n",
    "    \n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(cummulate_survival_ratio)\n",
    "plt.title('Survival rate change depending on range of Age', y=1.02)\n",
    "plt.ylabel('Survival rate')\n",
    "plt.xlabel('Range of Age(0~x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2aca7c4d07820b6022c423c8451bd8c1b474c15f"
   },
   "source": [
    "나이가 어릴 수록 생존률이 확실히 높은것을 확인할 수 있습니다.  \n",
    "이를 통해 나이도 중요한 피쳐임을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ec5007fd9153877bd44b72c82357443a346ce0d"
   },
   "source": [
    "지금까지 얻은 분석을 종합하자면,\n",
    "\n",
    "* 여자이거나\n",
    "* 나이가 어리고,\n",
    "* 클래스가 높을 수록\n",
    "\n",
    "생존확률이 높음을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "716f987c0352ec05455d05c2dadffdd9a762c822"
   },
   "source": [
    "### **2.5 Embarked**\n",
    "\n",
    "Embarked는 탑승한 항구를 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b26a4fc9dd23b1e8329181d5a81392ccb18a81b8"
   },
   "outputs": [],
   "source": [
    "df_train['Embarked'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "356b949e7ef585174c0e7100094bed6f9fd9ac25"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "df_train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=True).mean().sort_values(by='Survived', ascending=False).plot.bar(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "34d142d081b3d03e4f581f18c025f0e882d4f797"
   },
   "source": [
    "탑승한 곳에 따라 생존률의 차이가 많이 보이지는 않지만,  \n",
    "그래도 차이가 있으니 피쳐로 사용해보겠습니다.\n",
    "\n",
    "그럼 다른 피쳐와의 상관관계도 한번 알아보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c251ac72fbf840134f0e205d0986e5f52c01d4b"
   },
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(2, 2, figsize=(20,15))\n",
    "sns.countplot('Embarked', data=df_train, ax=ax[0,0])\n",
    "ax[0,0].set_title('(1) No. Of Passengers Boarded')\n",
    "sns.countplot('Embarked', hue='Sex', data=df_train, ax=ax[0,1])\n",
    "ax[0,1].set_title('(2) Male-Female Split for Embarked')\n",
    "sns.countplot('Embarked', hue='Survived', data=df_train, ax=ax[1,0])\n",
    "ax[1,0].set_title('(3) Embarked vs Survived')\n",
    "sns.countplot('Embarked', hue='Pclass', data=df_train, ax=ax[1,1])\n",
    "ax[1,1].set_title('(4) Embarked vs Pclass')\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4d890dbfc4e745466ceaad3fcdc229b4f9f535d8"
   },
   "source": [
    "* Figure(1) - 전체적으로 봤을 때, S 에서 가장 많은 사람이 탑승했습니다.\n",
    "* Figure(2) - C와 Q 는 남녀의 비율이 비슷하고, S는 남자가 더 많습니다.\n",
    "* Figure(3) - 생존확률이 S 경우 많이 낮은 걸 볼 수 있습니다. (이전 그래프에서 봤었습니다)\n",
    "* Figure(4) - Class 로 split 해서 보니, C가 생존확률이 높은건 클래스가 높은 사람이 많이 타서 그렇습니다. S는 3rd class 가 많아서 생존확률이 낮게 나옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "716f987c0352ec05455d05c2dadffdd9a762c822"
   },
   "source": [
    "### **2.6 Family - SibSp(형제 자매) + Parch(부모, 자녀)**\n",
    "\n",
    "`SibSp`와 `Parch`를 합치면 함께 탑승한 가족의 수가 될 것입니다.  \n",
    "이 두 피쳐를 더해서 새로운 피쳐 `FamilySize`를 만들어 보도록 하겠습니다.  \n",
    "(사실 새로운 피쳐를 만들어 내는 건 특성 공학쪽에서 다뤄야 하지만 일단 미리 보겠습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c202973135a04020e0bdad4ee03b9ea0fe746afb"
   },
   "outputs": [],
   "source": [
    "df_train['FamilySize'] = df_train['SibSp'] + df_train['Parch'] + 1 # 자신을 포함해야하니 1을 더합니다\n",
    "df_test['FamilySize'] = df_test['SibSp'] + df_test['Parch'] + 1 # 자신을 포함해야하니 1을 더합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c1a9b0f680ab471ae165963f8a190c4d3f98ee2"
   },
   "outputs": [],
   "source": [
    "print(\"Maximum size of Family: \", df_train['FamilySize'].max())\n",
    "print(\"Minimum size of Family: \", df_train['FamilySize'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05dd4ad9e702fb1acd3d5e912986f4272e33b97f"
   },
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1, 3, figsize=(40,10))\n",
    "sns.countplot('FamilySize', data=df_train, ax=ax[0])\n",
    "ax[0].set_title('(1) No. Of Passengers Boarded', y=1.02)\n",
    "\n",
    "sns.countplot('FamilySize', hue='Survived', data=df_train, ax=ax[1])\n",
    "ax[1].set_title('(2) Survived countplot depending on FamilySize',  y=1.02)\n",
    "\n",
    "df_train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=True).mean().sort_values(by='Survived', ascending=False).plot.bar(ax=ax[2])\n",
    "ax[2].set_title('(3) Survived rate depending on FamilySize',  y=1.02)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d1c1691be500989653d54a0aa9ef6505344e202"
   },
   "source": [
    "* Figure (1) - 가족크기가 1~11까지 있음을 볼 수 있습니다. 대부분 1명이고 그 다음으로 2, 3, 4명입니다.\n",
    "* Figure (2), (3) - 가족 크기에 따른 생존비교입니다. 가족이 4명인 경우가 가장 생존확률이 높습니다,  \n",
    "  가족수가 많아질수록, (5, 6, 7, 8, 11) 생존확률이 낮아지네요.  \n",
    "가족수가 너무 작아도(1), 너무 커도(5, 6, 8, 11) 생존 확률이 작네요. 3~4명 선에서 생존확률이 높은 걸 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "716f987c0352ec05455d05c2dadffdd9a762c822"
   },
   "source": [
    "### **2.7 Fare**\n",
    "\n",
    "해당 피쳐는 탑승 요금입니다. 연속적인 데이터이므로 한번 histogram을 그려보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6be9493369f166ada10b21365e39ba80735de451"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "g = sns.distplot(df_train['Fare'], color='b', label='Skewness : {:.2f}'.format(df_train['Fare'].skew()), ax=ax)\n",
    "g = g.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8a3364063fb9ea3480f161125211f977c3ab8f94"
   },
   "outputs": [],
   "source": [
    "# 특이하기도 train set 말고 test set에 Fare 피쳐에 널 값이 하나 존재하는 것을 확인할 수 있었습니다.\n",
    "# 그래서 평균 값으로 해당 널값을 넣어줍니다.\n",
    "df_test.loc[df_test.Fare.isnull(), 'Fare'] = df_test['Fare'].mean() # testset 에 있는 nan value 를 평균값으로 치환합니다.\n",
    "\n",
    "df_train['Fare'] = df_train['Fare'].map(lambda i: np.log(i) if i > 0 else 0)\n",
    "df_test['Fare'] = df_test['Fare'].map(lambda i: np.log(i) if i > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3581ff059a9e4d4b6d36acc7b211233c6757d65"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "g = sns.distplot(df_train['Fare'], color='b', label='Skewness : {:.2f}'.format(df_train['Fare'].skew()), ax=ax)\n",
    "g = g.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1211977568a7f933192e9ffeb31452e2d68dd463"
   },
   "source": [
    "log 를 취하니, 이제 비대칭성이 많이 사라진 것을 볼 수 있습니다.  \n",
    "우리는 이런 작업을 사용해 모델이 좀 더 좋은 성능을 내도록 할 수 있습니다.  \n",
    "사실 방금한 것은 **feature engineering** 에 들어가는 부분인데, 여기서 작업했습니다.  \n",
    "모델을 학습시키기 위해, 그리고 그 모델의 성능을 높이기 위해 feature 들에 여러 조작을 가하거나,  \n",
    "새로운 feature를 추가하는 것을 feature engineering 이라고 하는데,   \n",
    "우리는 다음 챕터에 그것을 살펴볼 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "716f987c0352ec05455d05c2dadffdd9a762c822"
   },
   "source": [
    "### **2.8 Cabin**\n",
    "\n",
    "이 feature 는 NaN 이 대략 80% 이므로, 생존에 영향을 미칠 중요한 정보를 얻어내기가 쉽지는 않습니다.  \n",
    "그러므로 우리가 세우려는 모델에 포함시키지 않도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a331d2438a317cc2401822515e4eee6eb2a5ebcf"
   },
   "outputs": [],
   "source": [
    "### Cabin 피쳐의 Null 비율 계산\n",
    "df_train[\"Cabin\"].isnull().sum() / df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58673a3e04173c7eee86f7fbd3a5499d6414a187"
   },
   "outputs": [],
   "source": [
    "df_train.head()[[\"PassengerId\", \"Cabin\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "716f987c0352ec05455d05c2dadffdd9a762c822"
   },
   "source": [
    "### **2.9 Ticket**\n",
    "\n",
    "이 feature 는 NaN 은 없습니다. 일단 string data 이므로 우리가 어떤 작업들을 해주어야  \n",
    "실제 모델에 사용할 수 있는데, 이를 위해선 사실 아이디어가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "03a5482147636b46965a47445cec65e01a55aab6"
   },
   "outputs": [],
   "source": [
    "df_train['Ticket'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07861f12ecf0783bcce71965d249fe0f9fcf676c"
   },
   "source": [
    "보시다시피, ticket number 는 매우 다양합니다.  \n",
    "우리는 여기서 어떤 특징을 이끌어내서 생존과 연결시킬 수 있을까요?  \n",
    "한번 생각을 해보시고 새로운 Feature를 만들어 모델의 성능을 끌어올리는 것도  \n",
    "재밌는 데이터 분석 과정 중 하나니 힘내서 생각하시고 즐겨보시길 바랍니다!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b7a217ed1eaf53a16d8d44bc432d408b59d51bc"
   },
   "source": [
    "## 3. 특성 공학 (Feature Engineering)\n",
    "---\n",
    "본격적인 feature engineering 을 시작해보겠습니다.\n",
    "\n",
    "***\"Garbage In, Garbage out\"* **  \n",
    "\n",
    "특성 공학을 통해 다양 피쳐를 만드시고, 정제하실 텐데   \n",
    "자기가 생각한 가정이 항상 타당한지 생각하시고 진행해 주시길 바랍니다!  \n",
    "아무리 좋은 모델도, 안좋은 데이터가 들어오면 안좋은 결과로 나오기 마련이니까요.\n",
    "\n",
    "가장 먼저, dataset 에 존재하는 null data를 채우려고 합니다.  \n",
    "아무 숫자로 채울 수는 없고, null data 를 포함하는 feature 의 statistics 를 참고하거나,  \n",
    "다른 아이디어를 짜내어 채울 수 있습니다.\n",
    "\n",
    "null data 를 어떻게 채우느냐에 따라 모델의 성능이 좌지우지될 수 있기 때문에, 신경써줘야할 부분입니다.\n",
    "\n",
    "특히, Feature engineering 은 실제 모델의 학습에 쓰려고 하는 것이므로,  \n",
    "**train 뿐만 아니라 test 도 똑같이 적용**해주어야 합니다. 잊지맙시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f1fbf3c153e06c96420de497b9fd57c0d4fc30d5"
   },
   "source": [
    "### **3.1 Fill Null**\n",
    "\n",
    "### 3.1.1 Fill Null in Age using title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "36398646bd0be4bfb4dc976df3178a9433ef2a08"
   },
   "outputs": [],
   "source": [
    "df_train[\"Age\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e3039cadd2b485c54f238b9cef3b9c14ccf5d668"
   },
   "source": [
    "Age 에는 null data가 177개나 있습니다. 이를 채울 수 있는 여러 아이디어가 있을 것인데,  \n",
    "여기서 우리는 **title + statistics** 를 사용해 보겠습니다.\n",
    "\n",
    "영어에서는 Miss, Mrr, Mrs 같은 title이 존재합니다.  \n",
    "각 탑승객의 이름에는 꼭 이런 title 이 들어가게 되는데 이를 사용해보겠습니다.\n",
    "\n",
    "pandas series 에는 data 를 string 으로 바꿔주는 `str` method,  \n",
    "거기에 정규표현식을 적용하게 해주는 `extract` method가 있습니다.  \n",
    "이를 사용하여 title 을 쉽게 추출할 수 있습니다. title을 `Initial` column에 저장하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ec125de0ac8b264578a2e81513eec084f3f34c1"
   },
   "outputs": [],
   "source": [
    "df_train['Initial']= df_train.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n",
    "df_test['Initial']= df_test.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc46e472cef2f0f581c39249dca85246f3199f78"
   },
   "source": [
    "pandas 의 crosstab 을 이용하여 우리가 추출한 Initial 과 Sex 간의 count 를 살펴봅시다.  \n",
    "혹시나 잘 못 이름이 적혀져 있거나 성별을 잘 못 분류 했을 수도 있으니깐요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0a05d63fd47dc31b481b138dc813ff3b7c00131a"
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df_train['Initial'], df_train['Sex']).T.style.background_gradient(cmap='summer_r') #Checking the Initials with the Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c3249bef6ecaee1b2dd2397b672b6e18fc8667d"
   },
   "outputs": [],
   "source": [
    "df_train['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],\n",
    "                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr', 'Mr'],inplace=True)\n",
    "\n",
    "df_test['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],\n",
    "                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr', 'Mr'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f287b092ecc0298e3a6135f090068869e419c8b"
   },
   "outputs": [],
   "source": [
    "df_train.groupby('Initial').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df26658c1e3450d755fa89ef3e11de92685322ff"
   },
   "outputs": [],
   "source": [
    "df_train.groupby('Initial')['Survived'].mean().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "752bc187098667514615e0128dabdb46f218429a"
   },
   "source": [
    "이제 본격적으로 Null 을 채울 것입니다. null data 를 채우는 방법은 정말 많이 존재합니다.  \n",
    "statistics 를 활용하는 방법도 있고, null data 가 없는 데이터를   \n",
    "기반으로 새로운 머신러닝 알고리즘을 만들어 예측해서 채워넣는 방식도 있습니다.  \n",
    "*(딥러닝에서 디노이징 오토인코더 라는 것이 있습니다!)*\n",
    "\n",
    "여기서는 statistics 를 활용하는 방법을 사용할 것입니다.  \n",
    "여기서 statistics 는 train data 의 것을 의미합니다.  \n",
    "우리는 언제나 test 를 unseen 으로 둔 상태로 놔둬야 하며,  \n",
    "train 에서 얻은 statistics 를 기반으로 test 의 null data 를 채워줘야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c161228f88248a191ea2a23c78a7b7c871ce7307"
   },
   "outputs": [],
   "source": [
    "df_train.groupby('Initial').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "61a26a6b55fd16c882449d3c8aa61815976f24bd"
   },
   "source": [
    "저희는 각 initial 그룹별 Age 평균 값을 사용해서 채워 넣도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "039494ac5beee3cdcabec452a587ccb23e942f14"
   },
   "outputs": [],
   "source": [
    "df_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Mr'),'Age'] = 33\n",
    "df_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Mrs'),'Age'] = 36\n",
    "df_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Master'),'Age'] = 5\n",
    "df_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Miss'),'Age'] = 22\n",
    "df_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Other'),'Age'] = 46\n",
    "\n",
    "df_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Mr'),'Age'] = 33\n",
    "df_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Mrs'),'Age'] = 36\n",
    "df_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Master'),'Age'] = 5\n",
    "df_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Miss'),'Age'] = 22\n",
    "df_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Other'),'Age'] = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7f927f008c9bbf173df571e0a71e094cb330129"
   },
   "outputs": [],
   "source": [
    "df_train.isnull().sum()[df_train.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "adc3e3a5df14f8d780134b9aa634fc9b6d66be6f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.isnull().sum()[df_test.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "29d79b68b7e46375fe768efcd5080d6b91a71318"
   },
   "source": [
    "train & test 셋에 Age의 널처리가 완전히 끝난 것을 보실 수 있습니다.  \n",
    "\n",
    "여기선 간단하게 Null을 채웠지만, 좀 더 다양한 방법을 쓴 예시들이 다른 커널에 존재합니다  \n",
    "https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling 보시면서 공부해보세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f1fbf3c153e06c96420de497b9fd57c0d4fc30d5"
   },
   "source": [
    "### 3.1.2 Fill Null in Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b056d336e623a57abd6ca6a60f53b0f6842cd5f"
   },
   "outputs": [],
   "source": [
    "print('Embarked has ', sum(df_train['Embarked'].isnull()), ' Null values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f6c3b06b22cd4729c42fc2468012c8c2b37b0608"
   },
   "source": [
    "Embarked 는 Null value 가 2개이고, S 에서 가장 많은 탑승객이 있었으므로,  \n",
    "간단하게 Null 을 S로 채우겠습니다.\n",
    "\n",
    "dataframe 의 `fillna` method 를 이용하면 쉽게 채울 수 있습니다.  \n",
    "여기서 inplace=True 로 하면 df_train 에 fillna 를 실제로 적용하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73680d3ec0492d05d49fabf8a881f94026abef57"
   },
   "outputs": [],
   "source": [
    "df_train['Embarked'].fillna('S', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d913136b90d5cd1e4227938d3c4951c4440fc13"
   },
   "outputs": [],
   "source": [
    "df_train.isnull().sum()[df_train.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f1fbf3c153e06c96420de497b9fd57c0d4fc30d5"
   },
   "source": [
    "### **3.2. Change Age(continuous to categorical)**\n",
    "\n",
    "Age 는 현재 continuous feature 입니다. 이대로 써도 모델을 세울 수 있지만,  \n",
    "Age 를 몇개의 group 으로 나누어 category 화 시켜줄 수 도 있습니다.  \n",
    "\n",
    "continuous 를 categorical 로 바꾸면 자칫 **information loss** 가 생길 수도 있습니다만,  \n",
    "본 튜토리얼에서는 다양한 방법을 소개하는 것이 목적이므로 진행하도록 하겠습니다.  \n",
    "\n",
    "방법은 여러가지가 있습니다.  \n",
    "dataframe 의 indexing 방법인 loc 를 사용하여 직접해줄 수 있고,  \n",
    "아니면 apply 를 사용해 함수를 넣어줄 수 있습니다.\n",
    "\n",
    "loc 예제는 위의 age mean 값을 채워 줬을 때 사용해 봤으므로,  \n",
    "여기에선 apply를 사용해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60ea83ae1b63f08054156a6346bb444054b508d3"
   },
   "outputs": [],
   "source": [
    "def category_age(x):\n",
    "    if x < 10:\n",
    "        return 0\n",
    "    elif x < 20:\n",
    "        return 1\n",
    "    elif x < 30:\n",
    "        return 2\n",
    "    elif x < 40:\n",
    "        return 3\n",
    "    elif x < 50:\n",
    "        return 4\n",
    "    elif x < 60:\n",
    "        return 5\n",
    "    elif x < 70:\n",
    "        return 6\n",
    "    else:\n",
    "        return 7    \n",
    "    \n",
    "df_train['Age_cat'] = df_train['Age'].apply(category_age)\n",
    "df_test['Age_cat'] = df_test['Age'].apply(category_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f38fcac51c3ff7b8873d3a2bb937b04caeba6fa"
   },
   "outputs": [],
   "source": [
    "df_train.groupby(['Age_cat'])['PassengerId'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ecc0656e1f32194fe15e4892d31fc4cf497ca9f"
   },
   "source": [
    "이제 파생 피쳐를 만들었으므로 원래 컬럼 Age 를 제거하는게 맞겠지만,  \n",
    "저희는 상관 관계가 높아도 모델의 설명력(성능)에 도움이 될 수 있으니  \n",
    "남겨 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b588fe89a5a63b2c31509e896611061bd5a64e61"
   },
   "source": [
    "### **3.3 Change Initial, Embarked and Sex (string to numerical)**\n",
    "\n",
    "현재 Initial 은 Mr, Mrs, Miss, Master, Other 총 5개로 이루어져 있습니다.  \n",
    "이런 카테고리로 표현되어져 있는 데이터를 모델에 인풋으로 넣어줄 때  \n",
    "우리가 해야할 것은 먼저 **컴퓨터가 인식할 수 있도록 수치화** 시켜야 합니다.\n",
    "\n",
    "`map` method 를 가지고 간단히 할 수 있습니다.  \n",
    "사전 순서대로 정리하여 mapping 하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60fb1e1795b4361992c717141196e7878718a6ed"
   },
   "outputs": [],
   "source": [
    "df_train['Initial'] = df_train['Initial'].map({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Other': 4})\n",
    "df_test['Initial'] = df_test['Initial'].map({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Other': 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c512537e924162b5e7c558521fd556c578f27c5"
   },
   "source": [
    "마찬가지 `Embarked`와 `Sex` 피쳐도 string이라 같은 과정을 거치겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "83f600f4e78bfca831a51b72429f3c8970a997d8"
   },
   "outputs": [],
   "source": [
    "df_train['Embarked'] = df_train['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "df_test['Embarked'] = df_test['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bbdb07a9f062ae76adf9524a76676174c35a5964"
   },
   "outputs": [],
   "source": [
    "df_train['Embarked'].isnull().any() , df_train['Embarked'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "012aa40365149312a50c4144f0e3cb2ed7b1d848"
   },
   "source": [
    "널이 없고, int형으로 잘 바뀐 것으로 보아 잘 변환 되었음을 확인할 수 있습니다.  \n",
    "마찬가지 `sex`에도 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6979c4ab333484ef78b2774600823288e8df169f"
   },
   "outputs": [],
   "source": [
    "df_train['Sex'] = df_train['Sex'].map({'female': 0, 'male': 1})\n",
    "df_test['Sex'] = df_test['Sex'].map({'female': 0, 'male': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b3008a35bbd84dfb3eff7846430485943e3f080f"
   },
   "source": [
    "여지껏 고생하셨습니다!! 엄청 많은걸 하셨어요!!\n",
    "\n",
    "이제 각 feature 간의 상관관계를 한번 보려고 합니다.  \n",
    "두 변수간의 Pearson correlation 을 구하면 (-1, 1) 사이의 값을 얻을 수 있습니다.   \n",
    "* -1로 갈수록 음의 상관관계,\n",
    "* 1로 갈수록 양의 상관관계를 의미하며\n",
    "* 0은 상관관계가 없다는 것을 의미합니다.\n",
    "\n",
    "따로 수식은 적지 않도록 하겠습니다. 검색만 하셔도 수식은 많이 나오니  \n",
    "궁금하시면 공부해보시걸 추천합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "abb34c83a3ff226f4e598af490db6870431255a3"
   },
   "outputs": [],
   "source": [
    "heatmap_data = df_train[['Survived', 'Pclass', 'Sex', 'Fare', 'Embarked', 'FamilySize', 'Initial', 'Age_cat', 'Age']] \n",
    "\n",
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize=(14, 12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(heatmap_data.astype(float).corr(), linewidths=0.1, vmax=1.0,\n",
    "           square=True, cmap=colormap, linecolor='white', annot=True, annot_kws={\"size\": 16})\n",
    "\n",
    "del heatmap_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a17871a09080a16a7f7e91c6e42f2975940e029"
   },
   "source": [
    "우리가 EDA에서 살펴봤듯이,  \n",
    "Sex 와 Pclass 가 Survived 에 상관관계가 어느 정도 있음을 볼 수 있습니다.  \n",
    "\n",
    "생각보다 fare 와 Embarked 도 상관관계가 있음을 볼 수 있습니다.  \n",
    "\n",
    "또한 우리가 여기서 얻을 수 있는 정보는   \n",
    "서로 강한 상관관계를 가지는 feature들이 없다는 것입니다.  \n",
    "(Age와 Age_cat 제외..)  \n",
    "이것은 우리가 모델을 학습시킬 때, 불필요한 feature 가 없다는 것을 의미합니다.  \n",
    "**1 또는 -1 의 상관관계를 가진 feature A, B 가 있다면, 우리가 얻을 수 있는 정보는 사실 하나일 거니까요.**\n",
    "\n",
    "이제 실제로 모델을 학습시키기 앞서서 data preprocessing (전처리)을 진행해보겠습니다.  \n",
    "거의 다 와갑니다 힙냅시다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1b6cef5431d3de82838a125ff3a15628b1d3d09"
   },
   "source": [
    "### **3.4 One-hot encoding on Initial and Embarked**\n",
    "\n",
    "수치화시킨 카테고리 데이터를 그대로 넣어도 되지만,   \n",
    "모델의 성능을 높이기 위해 one-hot encoding을 해줄 수 있습니다.\n",
    "\n",
    "수치화는 간단히 Master == 0, Miss == 1, Mr == 2, Mrs == 3, Other == 4 로 매핑해주는 것을 말합니다.\n",
    "\n",
    "One-hot encoding 은 위 카테고리를 아래와 같이 (0, 1) 로 이루어진 5차원의 벡터로 나타내는 것을 말합니다.\n",
    "\n",
    "|        | Initial_Master | Initial_Miss | Initial_Mr | Initial_Mrs | Initial_Other |\n",
    "|--------|----------------|--------------|------------|-------------|---------------|\n",
    "| Master | 1              | 0            | 0          | 0           | 0             |\n",
    "| Miss   | 0              | 1            | 0          | 0           | 0             |\n",
    "| Mr     | 0              | 0            | 1          | 0           | 0             |\n",
    "| Mrs    | 0              | 0            | 0          | 1           | 0             |\n",
    "| Other  | 0              | 0            | 0          | 0           | 1             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1b6cef5431d3de82838a125ff3a15628b1d3d09"
   },
   "source": [
    "이렇게 하게 되면 **각 클래스간 연관성을 Orthogonal(직교, 동일하게)** 만들 수 있습니다.  \n",
    "그래서 각 클래스 관의 상관 관계가 없어집니다.  \n",
    "\n",
    "> **More?**:\n",
    ">그냥 수치화 했을 경우 Master랑 Miss가 가까운 관계, Mrs와 Other이 가까운 관계로 해석될 수 있습니다.  \n",
    ">더 자세한 사항은 Label Encoding vs One-hot Encoding 으로 검색하시면 더 자세하게 알아 보실 수 있습니다.\n",
    "\n",
    "위와 같은 작업을 직접 코딩할 수도 있지만, pandas 의 `get_dummies` 를 사용하여 쉽게 해결 할 수 있습니다.  \n",
    "총 5개의 카테고리니, one-hot encoding 을 하고 나면 새로운 5개의 column 이 생겨납니다.  \n",
    "Initial 을 prefix로 두어서 구분이 쉽게 만들어 줍니다 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf7f3bc22ff48c16d34ef5799be5c709da55afbc"
   },
   "outputs": [],
   "source": [
    "df_train = pd.get_dummies(df_train, columns=['Initial'], prefix='Initial')\n",
    "df_test = pd.get_dummies(df_test, columns=['Initial'], prefix='Initial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6efc60d96d7c99ff83ecd6bcbafdbde9e756aef2"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc3a88398bc2abff8a652c19f03b730d538e5252"
   },
   "source": [
    "Embarked 에도 적용하겠습니다. Initial 때와 마찬가지로 one-hot encoding 을 사용해 표현하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3765868e31ba957dc300baeb1330ff43ccb20375"
   },
   "outputs": [],
   "source": [
    "df_train = pd.get_dummies(df_train, columns=['Embarked'], prefix='Embarked')\n",
    "df_test = pd.get_dummies(df_test, columns=['Embarked'], prefix='Embarked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b906041f67fc039ffde388daaa02e3a9d713a32f"
   },
   "source": [
    "아주 쉽게 one-hot encoding 을 적용했습니다.  \n",
    "다른 패키지로도 원핫 인코딩을 적용 시킬 수 있지만, 여기에선 다루지 않겠습니다.\n",
    "\n",
    "> **Tips**:\n",
    "> 가끔 category 가 100개가 넘어가는 경우가 있습니다.  \n",
    "이때 one-hot encoding을 사용하면 column이 100개가 생겨,  \n",
    "학습시 매우 버거울 경우가 있습니다. (**차원의 저주**라고 부르기도 합니다.)  \n",
    ">\n",
    ">이런 경우는 다른 방법을 사용하기도 하는데, 이는 나중에 다른 컴피티션 참가하시면  \n",
    "**차원의 저주** 키워드로 해결 방법을 모색하시길 바랍니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b25eedb8d16568c01d74216afae7f954b09ccba"
   },
   "source": [
    "### **3.5 Drop columns**\n",
    "\n",
    "마지막으로 필요없는 피쳐를 없애버릴 차례입니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de7a4fc12fd1b59b18477e5d0c3936f9098c7387"
   },
   "outputs": [],
   "source": [
    "df_train.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "df_test.drop(['PassengerId', 'Name',  'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21aa81ce287bb633c0afab8074bbdd572ba9e26c"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66b597188198187f870b3670bf6131589bca4c66"
   },
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "463380bd5d68380085dd87797ee9257474dc9552"
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e574670d900046309fd65740a5500cb178f7d48"
   },
   "outputs": [],
   "source": [
    "df_test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "869d490c0a4b850a1539ac0ec4ac8ce8fca7e2c6"
   },
   "source": [
    "보시다시피, train 의 Survived feature(target class)를 빼면  \n",
    "train, test 둘다 같은 columns 를 가진 걸 확인할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eb8772864b671e305f4d8a8777d7915d9a9f0132"
   },
   "source": [
    "## 4. 모델 개발 및 학습\n",
    "---\n",
    "\n",
    "드디어 모델 개발을 시작합니다!! 저는 이게 가장 재밌는 것 같습니다.. (TMI...)\n",
    "\n",
    "Sklearn 은 머신러닝의 처음부터 끝까지가 다 있습니다  \n",
    "feature engineering, preprocessing, 지도 학습 알고리즘, 비지도 학습 알고리즘,  모델 평가, 파이프라인 등...\n",
    "머신러닝에 관련된 모든 작업들이 손쉬운 인터페이스로 구현되어 있습니다.  \n",
    "데이터 분석 + 머신러닝을 하고싶다면, 이 라이브러리는 반드시 숙지해야합니다.\n",
    "\n",
    "> **Books?**  \n",
    "> * 파이썬 라이브러리를 활용한 머신러닝(Introduction to machine larning with Python)\n",
    "> * 핸즈온 머신러닝  \n",
    ">\n",
    "> sklearn 뿐만 아니라 머신러닝/딥러닝에 관한 기본을 잘 설명하고 있는 책들이니 공부하시는 것 권해드립니다!\n",
    "\n",
    "지금 타이타닉 문제는 target class(survived)가 있으며, target class 는 0, 1로 이루어져 있으므로(binary)  \n",
    "**binary classfication** 문제입니다.\n",
    "\n",
    "우리가 지금 가지고 있는 train set 의 survived를 제외한 input 을 가지고  \n",
    "모델을 최적화시켜서 각 샘플(탑승객)의 생존유무를 판단하는 모델을 만들어 냅니다.  \n",
    "그 후 모델이 학습하지 않았던 test set 을 input 으로 주어서 test set 의 각 샘플(탑승객)의 생존 유무를 예측해봅니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0a649574885da396b5da27d6a92b3bcde0bbe80d"
   },
   "outputs": [],
   "source": [
    "#importing all the required ML packages\n",
    "from sklearn.ensemble import RandomForestClassifier # 유명한 randomforestclassfier 입니다. \n",
    "from sklearn import metrics # 모델의 평가를 위해서 씁니다\n",
    "from sklearn.model_selection import train_test_split # traning set을 쉽게 나눠주는 함수입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd1dc0b110e7c6d35df7bd7e220a065b91d1fa77"
   },
   "source": [
    "### **4.1 Preparation - Split dataset into train, valid(dev), test set**\n",
    "\n",
    "가장 먼저, 학습에 쓰일 데이터와, target label(Survived)를 분리합니다.  \n",
    "`drop` 을 사용해 간단히 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e49675f5160d102a5b8bd7d1da7739dea02c12e3"
   },
   "outputs": [],
   "source": [
    "X_train = df_train.drop('Survived', axis=1).values\n",
    "target_label = df_train['Survived'].values\n",
    "X_test = df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b8a4a6df6d74692f65d3bbe65d662a5099acb189"
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5671788e502a4073e9efa5d7f3e221ea37508489"
   },
   "source": [
    "보통 train, test 만 언급되지만,  \n",
    "실제 좋은 모델을 만들기 위해서 우리는 valid(dev) set을 따로 만들어 모델 평가를 해봅니다.\n",
    "\n",
    "수능으로 비유하자면,  \n",
    "공부(train)를 하고 바로 수능(test)을 치는 것이 아니라,  \n",
    "공부(train)을 한 다음 모의고사(valid)를 거쳐 개인 학습정도를 확인하고  \n",
    "수능(test)을 치는 것과 비슷합니다.\n",
    "\n",
    "train_test_split 을 사용하여 쉽게 train 셋에서 train과 validation을 분리할 수 있습니다\n",
    "\n",
    "> **More?**  \n",
    "> Overfitting(과적합) 이슈를 나중에 공부해보시면 왜 Validation을 나누는지 이해할 수 있을 것입니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ba3b82ad7e31231e768962ee08224abb7dd0408"
   },
   "outputs": [],
   "source": [
    "X_tr, X_vld, y_tr, y_vld = train_test_split(X_train, target_label, test_size=0.2, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "deddd353f73d2d90e704d95c469bae1428f9b68d"
   },
   "outputs": [],
   "source": [
    "y_tr.shape, y_vld.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de1e65c6fd26116e35eba3d104bed11027f9a44d"
   },
   "source": [
    "sklearn 에서는 여러 머신러닝 알고리즘을 지원해줍니다.  \n",
    "열거하기엔 너무 많으므로, 직접 documentation에 들어가 보시길 추천합니다.  \n",
    "http://scikit-learn.org/stable/supervised_learning.html#supervised-learning  \n",
    "여기에 들어가시면 지원되는 알고리즘 수에 놀라실 겁니다.\n",
    "\n",
    "본 튜토리얼에서는 랜덤포레스트 모델을 사용하도록 하겠습니다.  \n",
    "\n",
    "랜덤포레스트는 결정트리기반 모델이며, 여러 결정 트리들을 앙상블한 모델입니다.  \n",
    "더 자세한 설명을 추천해드린 책이나 각종 블로그에 설명이 잘 되어있으니 찾아보시길 바랍니다!\n",
    "\n",
    "각 머신러닝 알고리즘에는 여러 파라미터들이 있습니다.  \n",
    "랜덤포레스트분류기도 n_estimators, max_features, max_depth, min_samples_split, min_samples_leaf 등 여러 파라미터들이 존재합니다.  \n",
    "이것들이 어떻게 세팅되냐에 따라 같은 데이터셋이라 하더라도 모델의 성능이 달라집니다.\n",
    "\n",
    "**파라미터 튜닝**은 시간, 경험, 알고리즘에 대한 이해 등이 필요합니다.   \n",
    "결국 많이 써봐야 모델도 잘 세울 수 있는 것이죠. 그래서 캐글을 추천합니다.   \n",
    "여러 데이터셋을 가지고 모델을 이리저리 써봐야 튜닝하는 감이 생길테니까요!\n",
    "\n",
    "일단 지금은 튜토리얼이니 파라미터 튜닝은 잠시 제쳐두기로 하고,   \n",
    "기본 default 세팅으로 진행하겠습니다.\n",
    "\n",
    "모델 객체를 만들고, `fit` 메소드로 학습시킵니다.  \n",
    "그런 후 valid set input 을 넣어주어 예측값(X_vld sample(탑승객)의 생존여부)를 얻습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5812ee39d7e6255ccf286ec50b1a95cdecd802f3"
   },
   "source": [
    "### **4.2 Model generation and prediction**\n",
    "\n",
    "이제 랜덤포레스트 모델을 생성하고 학습해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "68f4c0ac7b2f4933b2bb1f949d2b4d7c0ffe3204"
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_tr, y_tr)\n",
    "prediction = model.predict(X_vld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4bdbeb7b2d20eafb59dda630f2526f2b0cb9b59f"
   },
   "source": [
    "sklearn은 모델을 생성, 학습, 예측 하는데 3줄이면 됩니다!  \n",
    "오오오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "70638447a8d7c665e58e4489a064d6e54765e268"
   },
   "outputs": [],
   "source": [
    "print('총 {}명 중 {:.2f}% 정확도로 생존을 맞춤'.format(y_vld.shape[0], 100 * metrics.accuracy_score(prediction, y_vld)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64f734eb32fec761898bcf3e6284887df047fbbf"
   },
   "source": [
    "아무런 파라미터 튜닝도 하지 않았는데 79% 의 정확도가 나왔습니다. 고생하셨습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "70ef0f16ac6a43168dcbaa5075fff9a6ca74aa53"
   },
   "source": [
    "\n",
    "학습된 모델은 feature importance 를 가지게 되는데,  \n",
    "우리는 이것을 확인하여 지금 만든 모델이 어떤 feature 에 영향을 많이 받았는 지 확인할 수 있습니다.  \n",
    "\n",
    "쉽게 말해, `y = 4*x1 + 2*x2 + 1*x3` 을 생각하면,  \n",
    "우리는 x1이 결과값(10)에 큰 영향을 준다고 생각 할 수 있습니다. \n",
    "\n",
    "feature importance 는 4, 2, 1 을 이야기하며, x1이 가장 큰 값(4)를 가지므로,  \n",
    "이 모델에 x1 피쳐가 가장 큰 영향을 미친다고 말할 수 있습니다.\n",
    "\n",
    "학습된 모델은 기본적으로 feature importances 를 가지고 있어서 쉽게 그 수치를 얻을 수 있습니다.  \n",
    "pandas series 를 이용하면 쉽게 sorting 을 하여 그래프를 그릴 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "35174afd95808c74792148636672d12877f9ec9a"
   },
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "\n",
    "feature_importance = model.feature_importances_\n",
    "Series_feat_imp = Series(feature_importance, index=df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "54cdd50d690fab9b5b611fe9dfd207e358af60f6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "Series_feat_imp.sort_values(ascending=True).plot.barh()\n",
    "plt.xlabel('Feature importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e23c4ac8c9d29bdd87f17eeddd62394daff158bd"
   },
   "source": [
    "## 4.B keras를 사용한 NN 모델 개발\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc65d75e0231d04b1c69473f14e8af18d5b9a59c"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f338087063bef55741ec9575372c87a6f6435821"
   },
   "outputs": [],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(32,activation='relu',input_shape=(14,)))\n",
    "nn_model.add(Dropout(0.2))\n",
    "nn_model.add(Dense(64,activation='relu'))\n",
    "nn_model.add(Dropout(0.2))\n",
    "nn_model.add(Dense(32,activation='relu'))\n",
    "nn_model.add(Dropout(0.2))\n",
    "nn_model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "Loss = 'binary_crossentropy'\n",
    "nn_model.compile(loss=Loss,optimizer=Adam(),metrics=['accuracy'])\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4c7ce7ef2d2adab9c8befa18608bca6a8a9bf95",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = nn_model.fit(X_tr,y_tr,\n",
    "                    batch_size=64,\n",
    "                    epochs=500,\n",
    "                    validation_data=(X_vld, y_vld),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "acae6a6fbed4634ace7bc1e87079e044c7f76e66"
   },
   "outputs": [],
   "source": [
    "hists = [history]\n",
    "hist_df = pd.concat([pd.DataFrame(hist.history) for hist in hists], sort=True)\n",
    "hist_df.index = np.arange(1, len(hist_df)+1)\n",
    "fig, axs = plt.subplots(nrows=2, sharex=True, figsize=(16, 10))\n",
    "axs[0].plot(hist_df.val_acc, lw=5, label='Validation Accuracy')\n",
    "axs[0].plot(hist_df.acc, lw=5, label='Training Accuracy')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].grid()\n",
    "axs[0].legend(loc=0)\n",
    "axs[1].plot(hist_df.val_loss, lw=5, label='Validation MLogLoss')\n",
    "axs[1].plot(hist_df.loss, lw=5, label='Training MLogLoss')\n",
    "axs[1].set_ylabel('MLogLoss')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].grid()\n",
    "axs[1].legend(loc=0)\n",
    "fig.savefig('hist.png', dpi=300)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5f19d519b7265bfd1fdaed3886df5f80767abd74"
   },
   "source": [
    "## 5. 모델 예측 및 평가\n",
    "---\n",
    "\n",
    "이제 답안지를 제출해 볼 차례입니다!\n",
    "\n",
    "이제 모델이 학습하지 않았던(보지 않았던) 테스트셋을 모델에 주어서, 생존여부를 예측해보겠습니다.  \n",
    "이 결과는 실제로 submission(제출용) 이므로 결과는 leaderboard 에서 확인할 수 있습니다.  \n",
    "캐글에서 준 파일, gender_submission.csv 파일을 읽어서 제출 준비를 하겠습니다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "039d943823b66d7660f37e9eb0fd470cbcf7c89f"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a88bcec3b5169b6f056ab3721a41f8bbaa4cbe9"
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)\n",
    "submission['Survived'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcba515c0741eb904f59588e600f01ab9923da1b"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('my_first_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8229c310fcce77a5c81d37660a08798471ee232f"
   },
   "source": [
    "해당 파일을 제출하니 **0.75598** 정확도 스코어를 리더보드에서 얻을 수 있었습니다.  \n",
    "\n",
    "다른 특성 공학(Feature Engineering)과 다른 모델 방법들을 사용해보면 어떨까요?  \n",
    "나머지는 여러분의 몫인 것 같습니다!  화이팅!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "23c89f50e73939986531d4fe18cadf481cd6e087"
   },
   "source": [
    "## 5.B NN 예측 및 평가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6476ca8e7f34c2b07193751b229cc6ae7dd3aeff"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "prediction = nn_model.predict(X_test)\n",
    "prediction = prediction > 0.5\n",
    "prediction = prediction.astype(np.int)\n",
    "prediction = prediction.T[0]\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c2a1cc96ac6ca4ee1161b2b0cd2188c20abf6460"
   },
   "outputs": [],
   "source": [
    "submission['Survived'] = prediction\n",
    "submission.to_csv('my_nn_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6773c842243dba289781c059038119d7034060f0"
   },
   "source": [
    "NN 모델은 **0.77990** 리더보드가 나왔습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c42e347a68323a57ae3574cc20e835c9e401028"
   },
   "source": [
    "## 6. 결론\n",
    "---\n",
    "\n",
    "여러분은 엄청나게 빠른 시간내에 많은걸 체험하셨습니다.  \n",
    "이까지의 튜토리얼을 따라와 주셔서 감사합니다!! ^^\n",
    "\n",
    "이 튜토리얼 한번 만으로 모든걸 이해하신다면 대단하지만,  \n",
    "대부분 그렇지 않습니다! 걱정 안하셔도 되요!  \n",
    "\n",
    "중간 중간 팁들과 해당 코드를 더욱 분석하고, 다른 커널들도 보시면서,  \n",
    "어떻게하면 더 나은 성능을 낼 수 있을지 고민하시면,  \n",
    "더욱 나은 성능을 기대할 것이라 생각합니다.\n",
    "\n",
    "*본 튜토리얼을 따라하시다가, 혹시 문제가 있거나, 궁금한 사항이 있으면 언제든 말씀해주세요!*  \n",
    "저도 모르는 부분이 많아 같이 공유하고, 공부하면서 성장하는 캐글러가 되었으면 합니다.\n",
    "\n",
    "수고하셨습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9edf293117b6c1f3892ff956548ef28da382d243"
   },
   "source": [
    "## 7. 어떻게 공부하는게 좋을까요?\n",
    "---\n",
    "\n",
    "처음 시작할 때 많은 답답했던 부분이 이거였습니다.  \n",
    "사실 아직도 답은 못찾았고, 답은 없는 것 같습니다.  \n",
    "하지만 제가 주로 어떻게 하는지 알려드리자면,  \n",
    "\n",
    "* 일단 캐글 대회 참여해본다.\n",
    "    * 캐글 대회를 참여하고, 여러 분석한 커널들을 보면 다양한 최신 기술과 문제를 접근하는 방법을 배우게 됩니다.  \n",
    "    \n",
    "* 모르는 코드는 print하면서 분석하고, 안보고 다시 적어본다.\n",
    "    * 모르는 코드는 어느정도 분석하는 시간을 가져야 합니다.\n",
    "    \n",
    "* 모르는 함수는 공식 API 문서를 본다.\n",
    "    * sklearn, tensorflow, pandas, keras.. 와같은 유명 패키지들은 공식 문서가 상당히 잘 되어있습니다.\n",
    "    * 해당 함수는 여러 파라미터를 가지고 있는데 하나하나 보면서 어떤 역할을 하는지 훑어만 봐도 나중에 활용할 때 도움이 되었습니다.\n",
    "    \n",
    "* 기본 공부도 병행한다.\n",
    "    * 요즘 너무나도 좋은 강의, 책들이 많습니다. 이걸 보시면서 공부하시면 더 도움이 될 것 같습니다.\n",
    "    \n",
    "* 그래도 모르겠다면 구글링, github...\n",
    "    * 많은 블로그, 많은 오픈소스 안에 훌륭한 예제와 설명이 많습니다.  \n",
    "    빠르게 접근하기에 이만한 방법도 없을 것 같습니다.\n",
    "    \n",
    "<br> \n",
    "이렇게 작성할 수 있도록 도와주신 **캐글 코리아** 분들과,  \n",
    "항상 도움과 아이디어를 주시는 **Meetup: 대구 머신러닝 스터디**분들에게,  \n",
    "감사의 말씀을 올리며, \n",
    "\n",
    "마지막으로 도움이 되었던 강의, 책을 적어드리고 마치도록하겠습니다.  \n",
    "혹시 좋은 공부자료 있으시면 추천 부탁드릴께요.^^  \n",
    "\n",
    "고생하셨습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b7c9ff519ab4bf1019d0c171cb89b1f2911316d"
   },
   "source": [
    "### **강의 리스트**\n",
    "* [최성철 교수님 : 머신러닝을 위한 Python 워밍업](https://www.edwith.org/aipython)\n",
    "    * 처음 판다스, 넘파이 와 같은 라이브러리를 배우기에 좋았습니다.\n",
    "* [김성훈 교수님 : 머신러닝과 딥러닝 BASIC](https://www.edwith.org/others26)\n",
    "    * 처음 텐서플로우로 딥러닝 입문하는데 너무 좋았습니다.\n",
    "* [Andrew Ng 교수님 : 오픈 코세라 강의](https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w/playlists)\n",
    "    * 기본부터 찬찬히 공부하기에 너무 좋은 강의 입니다.  \n",
    "    강의 절반은 한글화가 되어 있으니 접근하시는데 무리가 없을 것입니다!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c0c3186411843f7a60c0d650a26668b0091f9f62"
   },
   "source": [
    "### **책 리스트**\n",
    "\n",
    "* 파이썬 라이브러리를 활용한 머신러닝(Introduction to machine larning with Python)\n",
    "    * 처음 캐글 코리아 분들과 스터디한 책입니다. 빠르게 캐글에 입문할 수 있도록 만든 책입니다.\n",
    "* 핸즈온 머신러닝\n",
    "    * 어렵긴 하지만 하나하나 도움이 되는 책인 것 같습니다. 아직 공부 중입니다...\n",
    "* 밑바닥 부터 시작하는 딥러닝\n",
    "    * 직접 파이썬으로 딥러닝 학습과정부터 한땀 한땀 구현하는게 재미있었습니다.  \n",
    "    구현하면서 배우는 걸 좋아하시는 분에게 추천해드립니다."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
